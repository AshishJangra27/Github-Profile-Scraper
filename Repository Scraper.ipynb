{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ce2c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62024ff4",
   "metadata": {},
   "source": [
    "### 1. Scraping Number of Repositories, Stars and Followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ab29938",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://github.com/AshishJangra27'\n",
    "res = requests.get(link)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "try:\n",
    "    sp = soup.find('nav', class_ = 'UnderlineNav-body width-full p-responsive').find_all('a')\n",
    "    for i in sp:\n",
    "        txt = i.text.replace('\\n','').replace(' ','')\n",
    "        if ('Repositories' in txt):\n",
    "            repo =  int(txt[12:])   \n",
    "        elif ('Stars' in txt):\n",
    "            strs =  int(txt[5:])\n",
    "except:\n",
    "    repo = 0\n",
    "    strs = 0\n",
    "    \n",
    "try:\n",
    "    sp         = soup.find_all('a', class_ = 'Link--secondary no-underline no-wrap')\n",
    "    followers  = int(sp[0].text.replace('\\n','').replace(' ','')[:-9])\n",
    "except:\n",
    "    followers  = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e905aeb",
   "metadata": {},
   "source": [
    "### 2. Number of Pages Required to scrape all repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7de7e99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = int(repo/30)        # Pages with 30 repositories\n",
    "\n",
    "if (repo % 30 != 0):\n",
    "    pages += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f872c3d",
   "metadata": {},
   "source": [
    "### 3. Scraping Repository data from Multiple Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b05c1909",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_dct = {}\n",
    "\n",
    "for page in range(1,pages+1):\n",
    "    \n",
    "    res = requests.get(link + '?page=' + str(page) + '&tab=repositories')\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "    for r in soup.find('div', id = 'user-repositories-list').find_all('li'):\n",
    "        try:\n",
    "            title = r.find('h3').find('a').text.replace('\\n','').strip()\n",
    "        except:\n",
    "            title = np.nan\n",
    "        try:\n",
    "            licence   = r.find_all('span', class_ = 'mr-3')[-1].text.strip()    \n",
    "        except:\n",
    "            licence   = 'Unlicenced'\n",
    "        try:\n",
    "            strs      = r.find('a', class_ = 'Link--muted mr-3').text.strip()\n",
    "        except:\n",
    "            strs      = 0\n",
    "        try:\n",
    "            language  = r.find('span', itemprop = 'programmingLanguage').text\n",
    "        except:\n",
    "            language  = np.nan\n",
    "        try:\n",
    "            des       = r.find('p').text.strip()\n",
    "        except:\n",
    "            des       = 'Not available'   \n",
    "        try:\n",
    "            repo_link = 'https://github.com' + r.find('a').get('href')\n",
    "        except:\n",
    "            repo_link = np.nan\n",
    "            \n",
    "        \n",
    "        download_link = repo_link + '/archive/refs/heads/main.zip'\n",
    "        \n",
    "        repository_dct[title] = {'repo_link' : repo_link, 'licence' : licence, 'stars' : strs, \n",
    "                             'description' : des, 'language' : language, 'download_link' : download_link}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e5d931",
   "metadata": {},
   "source": [
    "### 4. Scraping Followers Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee90e36c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "followers_dct = {}\n",
    "\n",
    "pages = int(followers/50)        # Pages with 30 repositories\n",
    "if (followers % 50 != 0):\n",
    "    pages += 1\n",
    "\n",
    "\n",
    "for page in range(1,pages+1):\n",
    "\n",
    "    res  = requests.get(link + '?page=' + str(page) + '&tab=followers')\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    sp = soup.find_all('div', class_ = 'd-table table-fixed col-12 width-full py-4 border-bottom color-border-muted')\n",
    "\n",
    "    for f in sp:\n",
    "\n",
    "        name_ = f.find_all('span')[0].text.strip()\n",
    "        id_   = f.find_all('span')[1].text.strip()\n",
    "\n",
    "        followers_dct[id_] = name_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfbfe09",
   "metadata": {},
   "source": [
    "### 5. Scraping Stargaze for Each Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7cb7552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 48/48 [00:26<00:00,  1.84it/s]\n"
     ]
    }
   ],
   "source": [
    "stargaze = []\n",
    "\n",
    "for k in tqdm(repository_dct.keys()):\n",
    "    \n",
    "    res = requests.get(repository_dct[k]['repo_link'] + '/stargazers')\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    \n",
    "    stargaze = [i.find('h3').find('a').text for i in soup.find('ol').find_all('li')]\n",
    "    \n",
    "    repository_dct[k]['stargaze'] = stargaze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d75bfd8",
   "metadata": {},
   "source": [
    "### 6. Combining Everything in One Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "103a46b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://github.com/AshishJangra27'\n",
    "res = requests.get(link)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "try:\n",
    "    img_link   = soup.find('img', style = 'height:auto;').get('src').split('?')[0]\n",
    "except:\n",
    "    img_link   = 'Not Available'\n",
    "    \n",
    "try:\n",
    "    name       = soup.find('h1').find_all('span')[0].text.strip()\n",
    "except:\n",
    "    name       = 'Not Available'\n",
    "    \n",
    "try:\n",
    "    user_id    = soup.find('h1').find_all('span')[1].text.strip()\n",
    "except:\n",
    "    user_id    = 'Not Available'\n",
    "    \n",
    "try:\n",
    "    bio        = soup.find('div',class_ = 'p-note user-profile-bio mb-3 js-user-profile-bio f4').text.strip().replace('\\n','')\n",
    "except:\n",
    "    bio        = 'Not Available'\n",
    "\n",
    "try:\n",
    "    sp         = soup.find_all('a', class_ = 'Link--secondary no-underline no-wrap')\n",
    "    followers  = int(sp[0].text.replace('\\n','').replace(' ','')[:-9])\n",
    "    following  = int(sp[1].text.replace('\\n','').replace(' ','')[:-9])\n",
    "except:\n",
    "    followers  = 'Not Available'\n",
    "    following  = 'Not Available'\n",
    "    \n",
    "try:\n",
    "    location   = soup.find('span',class_ = 'p-label').text.strip()\n",
    "except:\n",
    "    location   = 'Not Available'\n",
    "\n",
    "try:\n",
    "    sp = soup.find('nav', class_ = 'UnderlineNav-body width-full p-responsive').find_all('a')\n",
    "    for i in sp:\n",
    "        txt = i.text.replace('\\n','').replace(' ','')\n",
    "        if ('Repositories' in txt):\n",
    "            repo =  int(txt[12:])   \n",
    "        elif ('Projects' in txt):\n",
    "            proj =  int(txt[8:])\n",
    "        elif ('Packages' in txt):\n",
    "            pkgs =  int(txt[8:])\n",
    "        elif ('Stars' in txt):\n",
    "            strs =  int(txt[5:])\n",
    "except:\n",
    "    repo = 0\n",
    "    proj = 0\n",
    "    pkgs = 0\n",
    "    strs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25bbdce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = {}\n",
    "\n",
    "dct['name']      = name\n",
    "dct['user_id']   = user_id\n",
    "dct['bio']       = bio\n",
    "dct['location']  = location\n",
    "dct['followers'] = followers_dct\n",
    "dct['repo']      = repository_dct\n",
    "dct['img_link']  = img_link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7f9638",
   "metadata": {},
   "source": [
    "### 7. Final Walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49d66952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'user_id', 'bio', 'location', 'followers', 'repo', 'img_link'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554ffe8d",
   "metadata": {},
   "source": [
    "### 8. Saving the JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "027ff720",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = json.dumps(dct)\n",
    "\n",
    "fd = open('ashish.json','w')\n",
    "fd.write(txt)\n",
    "fd.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
